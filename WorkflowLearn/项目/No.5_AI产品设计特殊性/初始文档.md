# AI产品设计特殊性：全景导论

> **源真理版本**：v1.0 (AI生成，基于2025-2026年最新实践)
> **项目定位**：核心补齐——AI产品核心方法论

---

## 一、AI 产品与传统产品的核心区别

| 维度 | 传统软件产品 | AI 产品 |
|------|------------|--------|
| **输出确定性** | 输入确定，输出确定 | 输入确定，输出可能不同 |
| **错误模式** | Bug（可复现） | 幻觉（难预测） |
| **用户预期** | 功能符合预期 | 需要管理「AI 不是万能」的预期 |
| **边界清晰度** | 能做什么很明确 | 能力边界模糊 |
| **迭代方式** | 修复 Bug | 调优 Prompt/模型 |

> **核心认知**：AI 产品设计的核心挑战是「管理不确定性」——模型会犯错，且错误难以预测。

---

## 二、三级 Fallback 策略

### 2.1 什么是 Fallback？

**Fallback（兜底策略）** 是当主路径失败时的备选方案。在 AI 产品中，Fallback 尤其重要，因为模型可能在任何时候「答不上来」或「答错」。

### 2.2 三级 Fallback 设计

```
第一级：AI 模型直接回答
    ↓ (失败/不确定)
第二级：知识库/规则匹配
    ↓ (仍然失败)
第三级：转人工/提供其他帮助
```

### 2.3 各级触发条件

| 级别 | 触发条件 | 响应方式 |
|------|---------|---------|
| **一级** | 正常请求 | AI 直接生成回答 |
| **二级** | 置信度低 / 命中敏感词 / 检索无结果 | 匹配预设回答 / FAQ |
| **三级** | 二级也失败 / 用户表示不满 / 关键业务 | 转人工客服 / 留下联系方式 |

### 2.4 设计模板

```markdown
# Fallback 策略设计

## 一级：AI 回答
- 正常走 AI 生成流程
- 置信度 > 0.8 直接返回

## 二级：知识库匹配
触发条件：
- AI 置信度 < 0.8
- 检索不到相关文档
- 问题命中敏感词库

处理方式：
- 匹配 FAQ 库
- 返回标准回答
- 示例：「关于XX问题，请参考以下标准流程...」

## 三级：转人工
触发条件：
- 二级也无法匹配
- 用户连续追问3次
- 用户表达不满（检测负面情绪）
- 涉及投诉/退款等关键操作

处理方式：
- 提示「正在为您转接人工客服」
- 记录对话历史，便于人工接手
- 提供预估等待时间
```

---

## 三、置信度展示设计

### 3.1 为什么要展示置信度？

AI 模型对不同问题的「把握程度」不同。向用户传达这种不确定性，可以：
- 避免用户过度信任 AI
- 让用户知道何时需要人工核实
- 建立合理的用户预期

### 3.2 置信度的来源

| 来源 | 说明 |
|------|------|
| **模型输出概率** | 大模型生成时的 token 概率 |
| **检索相似度** | RAG 检索结果的相似度分数 |
| **规则判断** | 是否命中高风险词/敏感话题 |
| **业务逻辑** | 特定类型问题默认需要确认 |

### 3.3 展示方式选择

| 方式 | 适用场景 | 示例 |
|------|---------|------|
| **文字说明** | 大多数场景 | 「根据相关资料，可能的答案是...」|
| **标签/图标** | 需要快速识别 | 🟢 高可信 / 🟡 仅供参考 / 🔴 建议核实 |
| **百分比** | 技术用户 | 「置信度：85%」|
| **分级展示** | 复杂场景 | 高/中/低 三档 |

### 3.4 文案设计原则

**高置信度**：
> - 「根据XX规定，答案是...」
> - 「查询到明确记录...」

**中置信度**：
> - 「根据相关资料，可能是...」
> - 「基于类似案例推断...」

**低置信度**：
> - 「未找到直接相关的信息，以下仅供参考...」
> - 「建议咨询专业人员确认...」

### 3.5 设计陷阱

| 陷阱 | 后果 | 正确做法 |
|------|------|---------|
| 不展示置信度 | 用户过度信任 | 至少用文案暗示 |
| 总是说「不确定」| 用户不信任 | 只在真的不确定时说 |
| 置信度太技术化 | 用户看不懂 | 用日常语言表达 |

---

## 四、人机协同边界

### 4.1 核心问题

**哪些交给 AI 直接处理，哪些需要人工介入？**

这个边界的划分，直接决定了：
- 用户体验（效率 vs 准确性）
- 运营成本（AI 成本 vs 人工成本）
- 风险控制（错误的影响范围）

### 4.2 边界划分框架

| 维度 | AI 直接处理 | 需要人工审核 |
|------|-----------|-------------|
| **风险等级** | 低风险（查询、建议）| 高风险（审批、决策）|
| **准确率要求** | 可容忍偶尔错误 | 必须 100% 准确 |
| **业务影响** | 可逆、影响小 | 不可逆、影响大 |
| **法律合规** | 无合规要求 | 需要人工签字/确认 |

### 4.3 ToG 场景的特殊考量

在政务场景中，边界通常更保守：

| 场景 | 建议策略 |
|------|---------|
| 政策查询 | AI 可直接回答，但标注「仅供参考」|
| 办事指南 | AI 生成，但显示「以窗口为准」|
| 投诉受理 | AI 辅助分类，人工确认受理 |
| 审批流程 | AI 预审，人工最终决定 |
| 信息公开 | 必须人工审核后发布 |

### 4.4 设计模板

```markdown
# 人机协同边界设计

## AI 直接处理
- 信息查询类：政策查询、进度查询、常见问题
- 简单操作类：预约、取号、表单预填
- 条件：置信度高、无敏感信息

## AI 辅助 + 人工确认
- 分类判断：投诉分类、工单派发
- 初步审核：材料完整性检查
- 流程：AI 给出建议 → 人工一键确认/修改

## 必须人工处理
- 最终审批：行政许可、资金审批
- 敏感操作：删除、撤销、处罚
- 特殊情况：超出 AI 能力范围
```

---

## 五、用户预期管理

### 5.1 为什么预期管理很重要？

用户对 AI 的预期往往是两极化的：
- **过高预期**：认为 AI 万能 → 失望
- **过低预期**：认为 AI 没用 → 不愿尝试

**目标**：建立「合理预期」——知道 AI 能做什么、不能做什么。

### 5.2 预期管理触点

| 触点 | 设计要点 |
|------|---------|
| **首次进入** | 简单说明 AI 助手的能力范围 |
| **输入引导** | 提供示例问题，暗示能力边界 |
| **回答开头** | 用文案暗示回答的确定性 |
| **回答结尾** | 提供后续选项（追问/转人工）|
| **错误时** | 承认局限，提供替代方案 |

### 5.3 引导文案设计

**首次进入**：
> 「你好，我是XX助手。我可以帮你查询政策、解答常见问题、指导办事流程。
> 如果遇到复杂问题，我会帮你转接人工客服。」

**输入提示**：
> 「你可以问我：
> - 如何办理XX证件？
> - XX政策的具体规定是什么？
> - 我的申请进度如何？」

**回答不确定时**：
> 「关于这个问题，我找到了一些相关信息，但可能不完全准确。
> 建议你 [查看官方文档] 或 [咨询人工客服] 确认。」

---

## 六、错误反馈流程

### 6.1 AI 出错的类型

| 类型 | 表现 | 用户感受 |
|------|------|---------|
| **答非所问** | 回答与问题无关 | 困惑 |
| **信息错误** | 回答有事实错误 | 被误导 |
| **格式问题** | 输出格式不对 | 不专业 |
| **无法回答** | 说「我不知道」| 失望 |
| **重复/啰嗦** | 反复说同样的话 | 烦躁 |

### 6.2 错误反馈设计

**承认错误**：
> 「抱歉，我可能理解错了你的问题。你是想问...吗？」

**提供替代**：
> 「这个问题我暂时无法准确回答。你可以：
> 1. 换个方式描述问题
> 2. 查看相关文档
> 3. 联系人工客服」

**收集反馈**：
> 「这个回答对你有帮助吗？👍 👎」
> 「你期望的回答是什么？[输入框]」

### 6.3 反馈闭环

```
用户反馈「回答不对」
    ↓
记录：问题 + AI回答 + 用户反馈
    ↓
定期分析：识别常见错误模式
    ↓
优化：调整 Prompt / 补充知识库 / 加入 Fallback
    ↓
验证：相同问题是否改善
```

---

## 七、综合应用：AI风险研判助手

### 7.1 产品定位

为城市应急管理人员提供 AI 辅助的风险研判工具，结合你的风险研判方法论（动因链/关系链/传播链）。

### 7.2 方案设计

```markdown
# AI风险研判助手 产品方案

## 一、产品定位
辅助应急管理人员进行风险研判，不替代人工决策

## 二、核心功能

### 2.1 风险快速研判
- 输入：事件描述、传感器数据
- AI处理：提取关键信息 → 匹配历史案例 → 初步风险评估
- 输出：风险等级（建议）+ 依据 + 类似案例

### 2.2 动因链分析
- 输入：事件
- AI处理：基于知识库推理可能的原因链
- 输出：原因链图 + 每个环节的置信度
- 人工：确认/修改原因链

### 2.3 传播预测
- 输入：当前态势
- AI处理：基于模型预测可能的发展趋势
- 输出：预测路径 + 概率 + 建议措施
- 人工：选择应对方案

## 三、人机协同设计

| 功能 | AI 职责 | 人工职责 |
|------|--------|---------|
| 信息提取 | 自动提取关键信息 | 补充/修正 |
| 风险评估 | 给出建议等级 | 最终确定 |
| 原因分析 | 提供候选原因 | 确认因果链 |
| 决策建议 | 提供方案选项 | 选择并执行 |

## 四、Fallback 设计

| 场景 | Fallback |
|------|---------|
| 无法识别事件类型 | 提示手动选择类型 |
| 无类似历史案例 | 提供通用研判框架 |
| 预测置信度低 | 明确提示「建议人工研判」|
| 关键决策 | 强制人工确认 |

## 五、置信度展示

- 风险等级旁显示：🟢 高可信 / 🟡 仅供参考 / 🔴 建议人工
- 原因链每个节点显示置信度
- 预测结果显示概率区间

## 六、用户引导

首次使用：
> 「我是风险研判助手，可以帮你快速分析事件、评估风险、预测发展。
> 所有建议仅供参考，最终决策请以人工判断为准。」

研判结果：
> 「基于当前信息，初步评估风险等级为【中】。
> 依据：[原因列表]
> 类似案例：[案例列表]
> ⚠️ 建议结合现场情况确认」

## 七、评估指标

| 指标 | 目标 |
|------|------|
| 研判时间缩短 | 较人工提升 50%+ |
| 用户采纳率 | AI建议被采纳 ≥ 70% |
| 准确率 | 事后验证正确 ≥ 80% |
| 用户满意度 | ≥ 4.0/5.0 |
```

---

## 八、学习节点说明

基于以上内容，本项目划分为 6 个学习节点：

### A层：核心设计模式

| 节点 | 核心内容 | 验证输出 |
|------|---------|---------|
| **A1** | 三级 Fallback 策略：设计兜底方案 🚧 | 为城市政务问答助手设计三级 Fallback 方案 |
| **A2** | 置信度展示设计：表达不确定性 | 为风险研判结果设计置信度展示方案（含文案）|
| **A3** | 人机协同边界：划分 AI 与人工职责 | 为城市事件处置系统划分人机协同边界 |

### B层：用户体验

| 节点 | 核心内容 | 验证输出 |
|------|---------|---------|
| **B1** | 用户预期管理：建立合理预期 | 为政务 AI 助手设计首次使用引导文案 |
| **B2** | 错误反馈流程：处理 AI 出错 | 设计 AI 回答错误时的反馈流程 + 文案 |

### C层：综合应用

| 节点 | 核心内容 | 验证输出 |
|------|---------|---------|
| **C1** | 综合：AI风险研判助手 | 完整产品方案 |

---

## 九、思考问题

读完这篇导论，尝试不看上文回答：

1. **AI 产品与传统产品的核心区别是什么？这对产品设计有什么影响？**

2. **三级 Fallback 策略分别是什么？各在什么情况下触发？**

3. **在 ToG 场景中，人机协同边界应该如何划分？为什么？**

---

> **下一步**：进入 A1 节点，学习三级 Fallback 策略设计。
