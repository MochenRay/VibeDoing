# 大模型基础认知：全景导论

> **源真理版本**：v1.0 (AI生成，基于2025-2026年最新实践)
> **项目定位**：核心补齐——底层认知框架

---

## 一、什么是大模型？

**大语言模型（Large Language Model, LLM）** 是一种基于深度学习的 AI 系统，通过学习海量文本数据，具备了理解和生成人类语言的能力。

**「大」在哪里？**
- **参数量大**：GPT-4 约 1.8 万亿参数，通义千问 Qwen3-Max 超万亿参数
- **训练数据大**：数万亿 Token 的文本数据
- **能力广**：一个模型能做翻译、写作、编程、推理等多种任务

---

## 二、Token：大模型的「阅读单位」

### 2.1 Token 是什么？

Token 是大模型处理文本的基本单位。它不是字符，也不是单词，而是介于两者之间的「子词」。

**英文示例**：
```
"Hello world" → ["Hello", " world"] → 2 tokens
"unhappiness" → ["un", "happiness"] → 2 tokens
```

**中文示例**：
```
"你好世界" → ["你好", "世界"] 或 ["你", "好", "世", "界"] → 2-4 tokens
```

### 2.2 中文 Token 估算

> **经验法则**：500 字中文 ≈ 350-400 Token

| 内容类型 | 字数 | 约Token数 |
|---------|------|----------|
| 一句话 | 20字 | 15-20 |
| 一段话 | 100字 | 70-100 |
| 一页A4 | 500字 | 350-400 |
| 一篇文章 | 2000字 | 1400-1600 |

### 2.3 为什么 Token 很重要？

1. **计费依据**：API 按 Token 收费
2. **能力边界**：Context Window 限制了单次能处理的 Token 数
3. **成本估算**：产品经理需要能估算 AI 功能的成本

---

## 三、Context Window：大模型的「工作记忆」

### 3.1 什么是 Context Window？

Context Window（上下文窗口）是大模型单次能处理的最大 Token 数，包括输入和输出。

**类比**：就像人的工作记忆有限，你不可能同时记住一本书的所有内容。

### 3.2 主流模型的 Context Window（2026年1月）

| 模型 | Context Window | 约等于 |
|------|---------------|-------|
| **Gemini 3 Pro** | 1M tokens | 约一本书（约2500页）|
| **Claude Opus 4.5** | 200K tokens（1M beta测试中）| 约500页文档 |
| **GPT-4o** | 128K tokens | 约300页文档 |
| **豆包 Doubao-Seed-1.8** | 256K tokens | 约600页文档 |
| **Kimi** | 200K tokens | 约500页文档 |
| **GPT-4o Mini** | 128K tokens | 约300页文档 |

### 3.3 为什么不能把整本书扔给模型？

即使 Context Window 足够大，也有问题：

1. **成本问题**：Token 越多，费用越高
2. **注意力稀释**：信息太多，模型可能「忘记」关键内容
3. **响应变慢**：处理时间与 Token 数成正比
4. **准确率下降**：关键信息被淹没在大量文本中

**正确做法**：用 RAG 检索相关内容，只把必要信息放入 Context

---

## 四、Temperature：控制输出的「创意程度」

### 4.1 Temperature 是什么？

Temperature（温度）是控制模型输出随机性的参数，范围通常是 0-2。

```
Temperature 低 (0-0.3) → 输出确定、一致、保守
Temperature 高 (0.7-1.2) → 输出多样、创意、随机
```

### 4.2 不同 Temperature 的效果

**同一问题，不同 Temperature**：

问题：「给这篇文章起个标题」

| Temperature | 输出风格 | 示例 |
|------------|---------|------|
| 0 | 最可能的答案，每次一样 | 「关于XX的分析报告」|
| 0.5 | 较稳定，偶有变化 | 「深度解析：XX的前景」|
| 1.0 | 多样化，有创意 | 「当XX遇上YY：一场意外的碰撞」|

### 4.3 如何选择 Temperature？

| 场景 | 推荐值 | 理由 |
|------|-------|------|
| **数据提取** | 0 | 需要准确、一致 |
| **分类判断** | 0-0.3 | 需要稳定结果 |
| **文案写作** | 0.7-1.0 | 需要创意和多样性 |
| **代码生成** | 0-0.3 | 需要正确性 |
| **头脑风暴** | 1.0-1.2 | 需要发散思维 |

### 4.4 其他采样参数

| 参数 | 作用 | 常用值 |
|------|------|-------|
| **top_p** | 控制候选词范围（累积概率） | 0.9-1.0 |
| **top_k** | 只考虑前 k 个候选词 | 50-100 |
| **frequency_penalty** | 惩罚重复词 | 0-1 |
| **presence_penalty** | 鼓励新话题 | 0-1 |

### 4.5 Temperature 的技术原理（面试追问）

> 面试官常问：「为什么 Temperature 高会更随机？」这一节帮你接住追问。

**核心机制**：Temperature 控制 softmax 函数的「平滑度」。

模型预测下一个词时，会给每个候选词一个「分数」（logits），然后用 softmax 转成概率：

```
P(word) = exp(logit / T) / Σexp(logits / T)
```

- **T=1（默认）**：正常概率分布
- **T<1（如0.3）**：高分词概率更高，低分词概率被「压制」→ 输出更确定
- **T>1（如1.2）**：概率分布更平均，低分词也有机会被选中 → 输出更随机

**数值示例**：

假设模型对「好」「棒」「差」三个词的原始分数是 [2.0, 1.0, 0.5]

| Temperature | 「好」概率 | 「棒」概率 | 「差」概率 | 效果 |
|-------------|----------|----------|----------|------|
| 0.5 | 78% | 18% | 4% | 几乎总是选「好」|
| 1.0 | 54% | 30% | 16% | 偶尔选其他 |
| 2.0 | 40% | 33% | 27% | 三个都有机会 |

**直觉理解**：
- 低温度 = 「只选最有把握的」
- 高温度 = 「给冷门选项一点机会」

**面试回答模板**：
> Temperature 控制的是 softmax 后的概率分布。低温度让高概率 token 更突出，输出更确定；高温度让概率分布更平均，给低概率 token 采样机会，所以输出更随机。本质上是在「确定性」和「多样性」之间做权衡。

---

## 五、模型幻觉（Hallucination）

### 5.1 什么是幻觉？

**幻觉（Hallucination）** 是指大模型生成看起来合理、但实际上是错误或虚构的内容。

> **核心认知**：大模型不是「知道」答案，而是「预测」最可能的下一个词。它没有真正的知识库，只有统计规律。

### 5.2 幻觉的三种类型

| 类型 | 描述 | 示例 |
|------|------|------|
| **事实错误** | 说出不存在的事实 | 「爱因斯坦获得了1922年诺贝尔化学奖」|
| **虚构引用** | 编造不存在的来源 | 「根据《自然》杂志2023年的研究...」（实际没有这篇论文）|
| **逻辑矛盾** | 前后不一致 | 先说A>B，后说B>A |

### 5.3 幻觉的成因

1. **概率生成**：模型是按概率生成文本，不是查询事实
2. **训练数据局限**：数据有截止日期，且可能包含错误
3. **缺乏事实核查**：模型没有「核对」机制
4. **「讨好」倾向**：模型倾向于给出答案，而不是说「不知道」

### 5.4 三个典型幻觉案例

**案例1：虚构人物**
```
问：张三丰是什么时候发明太极拳的？
答：张三丰于1368年创立太极拳...

问题：张三丰是传说人物，是否真实存在、何时创拳都有争议
```

**案例2：虚构论文**
```
问：有哪些关于XX的重要研究？
答：Smith et al. (2021) 在《Nature》发表了...

问题：这篇论文可能根本不存在
```

**案例3：错误计算**
```
问：15% of 85 是多少？
答：15% of 85 = 12.25（错误，正确答案是 12.75）

问题：大模型的数学计算不可靠
```

### 5.5 如何缓解幻觉？

| 方法 | 说明 |
|------|------|
| **RAG** | 检索真实文档作为依据 |
| **要求引用** | 让模型标注来源 |
| **降低 Temperature** | 减少随机性 |
| **人工核查** | 关键信息必须人工验证 |
| **明确边界** | 告诉模型「不知道就说不知道」|

---

## 六、主流模型对比（2026年1月）

### 6.1 模型能力对比

| 模型 | 优势 | 劣势 | 适用场景 |
|------|------|------|---------|
| **GPT-4o / GPT-5.2** | 综合能力强、生态最完善、多模态 | 国内直接访问受限 | 复杂推理、代码、多模态 |
| **Claude Opus 4.5** | 推理质量顶尖、长文本、安全性 | 价格较高（$5/$25 per 1M）| 复杂分析、代码、文档 |
| **Claude Sonnet 4.5** | 速度快、性价比好（$3/$15 per 1M）| 复杂推理略逊 Opus | 日常对话、快速迭代 |
| **Gemini 3 Pro** | 1M 超长上下文、Deep Research、多模态 | 中文能力略弱 | 超长文档、实时搜索研究 |
| **文心一言 4.5 Turbo** | 中文优化、国内合规、响应快 | 英文能力弱于海外模型 | 国内B端项目 |
| **通义千问 Qwen3-Max** | 开源生态、万亿参数、私有化部署 | 生态完善度不如GPT | 私有化部署、二次开发 |
| **豆包 Doubao-Seed-1.8** | 256K 长上下文、国内可用 | 知名度较低 | 长文档处理、国内项目 |
| **Kimi** | 200K 上下文、产品体验好 | 综合能力弱于头部模型 | 长文档处理、C端产品 |

### 6.2 价格参考（2026年1月）

| 模型 | 输入价格 | 输出价格 | 备注 |
|------|---------|---------|------|
| GPT-4o | $2.50/1M | $10.00/1M | 主力模型 |
| GPT-4o Mini | $0.15/1M | $0.60/1M | 轻量任务首选 |
| Claude Opus 4.5 | $5.00/1M | $25.00/1M | 最强推理 |
| Claude Sonnet 4.5 | $3.00/1M | $15.00/1M | 性价比之选 |
| Gemini 3 Pro | 按用量计费 | - | 1M 上下文免费额度 |

### 6.3 选型建议

| 场景 | 推荐模型 | 理由 |
|------|---------|------|
| **国内ToG项目** | 文心一言/通义千问/豆包 | 合规、数据安全、国产化要求 |
| **复杂推理任务** | Claude Opus 4.5 / GPT-4o | 推理能力顶尖 |
| **超长文档处理** | Gemini 3 Pro | 1M 上下文窗口 |
| **实时信息研究** | Gemini 3 Pro | Deep Research 多轮搜索 |
| **成本敏感** | GPT-4o Mini / Claude Sonnet | 性价比高 |
| **代码生成** | Claude Opus 4.5 / GPT-4o | 代码能力强 |
| **私有化部署** | 通义千问 Qwen3 | 开源、可本地部署 |

---

## 七、大模型工作原理概述（产品经理视角）

### 7.1 核心原理：预测下一个词

大模型的本质是：**给定前面的文本，预测下一个最可能的词**。

```
输入：今天天气真
预测：好（概率0.4）、不错（0.3）、差（0.1）、热（0.1）...
选择：好

输入：今天天气真好
预测：，（0.3）、！（0.2）、适合（0.15）...
选择：，
```

通过反复预测，模型「生成」了看起来连贯的文本。

### 7.2 训练流程（简化版）

```
1. 预训练（Pre-training）
   - 用海量文本训练模型预测下一个词
   - 学习语言规律、世界知识
   - 需要巨大算力，持续数月

2. 微调（Fine-tuning）
   - 用特定任务的数据调整模型
   - 让模型更擅长特定类型的任务

3. RLHF（人类反馈强化学习）
   - 人类评价模型输出的好坏
   - 根据反馈调整模型行为
   - 让模型更符合人类偏好
```

### 7.3 产品经理需要知道的

| 概念 | 产品含义 |
|------|---------|
| **预训练数据** | 决定模型的「基础知识」|
| **微调** | 可以让模型适应特定领域/风格 |
| **RLHF** | 决定模型的「行为规范」|
| **Context Window** | 限制了单次能处理的信息量 |
| **Temperature** | 控制输出的确定性/创意程度 |

---

## 八、5分钟讲解脚本

> 以下是向外行解释「大模型怎么工作」的标准话术

**开场**：
> 「大模型本质上是一个超级文本预测器。它读了互联网上几乎所有的文字，学会了语言的规律。」

**核心原理**：
> 「它的工作方式很简单：给它一段话的开头，它预测最可能的下一个词，然后再预测下一个，反复进行。比如你输入『今天天气真』，它可能预测『好』的概率最高，就输出『好』。」

**为什么看起来「懂」很多**：
> 「因为它读了太多文字，所以知道『火星是红色的』『水的化学式是H2O』这类知识。但它不是真的『知道』，而是统计规律告诉它这样回答概率最高。」

**局限性**：
> 「所以它有两个明显的问题：一是会『一本正经地胡说八道』，因为它追求的是『像』而不是『对』；二是它的知识有截止日期，不知道最新的事。」

**总结**：
> 「你可以把它想象成一个读了很多书的人，但这个人没有记忆、没有常识核查能力、也不知道自己不知道什么。用好它的关键是：知道它能做什么、不能做什么。」

---

## 九、学习节点说明

基于以上内容，本项目划分为 5 个学习节点：

### A层：基础概念

| 节点 | 核心内容 | 验证输出 |
|------|---------|---------|
| **A1** | Token 与上下文窗口：理解 Token 估算和 Context 限制 🚧 | 估算一份 PRD 的 Token 消耗 + Context 占用分析 |
| **A2** | Temperature 与采样：理解参数对输出的影响 | 为 3 个业务场景设计 Temperature 配置并说明理由 |
| **A3** | 模型幻觉与成因：理解幻觉类型和缓解方法 | 3 个案例归因 + 从 ToG 项目中找一个可能产生幻觉的场景 |

### B层：综合理解

| 节点 | 核心内容 | 验证输出 |
|------|---------|---------|
| **B1** | 主流模型对比：了解各模型特点 | 为「城市大脑」类项目选型并论述理由 |
| **B2** | 大模型工作原理概述：能给外行讲清楚 🚧 | 5 分钟讲解脚本（含向甲方领导汇报版本）|

---

## 十、思考问题

读完这篇导论，尝试不看上文回答：

1. **500字中文大约消耗多少 Token？为什么产品经理需要能估算 Token？**

2. **Temperature 参数如何影响输出？在数据提取和文案写作场景下，应该分别设置多少？**

3. **模型幻觉是什么？为什么会产生？如何缓解？**

---

> **下一步**：进入 A1 节点，学习 Token 与上下文窗口的概念。
