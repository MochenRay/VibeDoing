# A1. Token与上下文窗口

---
status: mastered
mastered_date: 2026-01-22
last_modified: 2026-01-22
next_review: 2026-02-02
last_reviewed: 2026-01-26
---

## 🎯 核心概念

### Token 估算
- **中文经验法则**：1 汉字 ≈ 0.7-0.8 Token
- **快速估算**：500 字中文 ≈ 350-400 Token

### Context Window
- 大模型单次能处理的最大 Token 数（输入 + 输出）
- 主流模型：GPT-4o 128K、Claude 200K、Gemini 1M

### ⚠️ 注意力稀释（Lost in the Middle）

> **重点记忆**：即使 Context 塞得下，也不代表模型能「记住」所有内容。

- 大模型对 Context **中间部分**的关注度会下降
- **开头和结尾**的信息更容易被「记住」
- 这不是幻觉，是注意力机制的特性

**实践含义**：
- 关键信息放在 Prompt 开头或结尾
- 长文档用 RAG 检索相关片段，而非全文塞入

---

## 💡 验证高光

**场景分析**：15000 字政策文档 + GPT-4o（128K）
- Token 估算：10500-12000 Token
- 技术可行：远低于 128K 上限
- 实践建议：仍推荐 RAG 方案，因为注意力稀释 + 未来扩展性

---

## 📚 延伸

- 为什么产品经理需要估算 Token？→ 计费、Context 规划、成本控制
- RAG 的核心价值：只把相关信息放入 Context，避免注意力稀释
